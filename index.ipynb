{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# K-Nearest Neighbors\n", "\n", "![wilson](img/wilson.jpg)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["KNearest Neighbors is our second classification algorithm in our toolbelt added to our logistic regression classifier.\n", "\n", "If we remember, logistic regression is a supervised, parametric, discriminative model.\n", "\n", "KNN is a supervised, non-parametric, discriminative, lazy-learning algorithm.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Let's load in our trusty Titanic dataset\n", "\n", "![titanic](https://media.giphy.com/media/uhB0n3Eac8ybe/giphy.gif)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### For visualization purposes, we will use only two features for our first model"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Titanic is a binary classification problem, with our target being the Survived feature"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Then perform another tts, and put aside the test set from above until the end\n", "\n", "We will hold of from KFold or crossval for now, so that our notebook is more comprehensible."]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Quick review of confusion matrix and our metrics: \n", "  "]}, {"cell_type": "markdown", "metadata": {}, "source": ["# KNN: Under the Hood"]}, {"cell_type": "markdown", "metadata": {}, "source": ["For visualization purposes, let's pull out a small subset of our training data, and create a model using only two dimensions: Age and Fare.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The KNN algorithm works by simply storing the training set in memory, then measuring the distance from the training points to a a new point.\n", "\n", "Let's drop a point from our validation set into the plot above."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Then, KNN finds the K nearest points. K corresponds to the n_neighbors parameter defined when we instantiate the classifier object."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's fit our training data, then predict what our validation point will be based on the closest 1 neighbor."]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Chat poll: What will our 1 neighbor KNN classifier predict our new point to be?\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["When we raise the value of K, KNN acts democratically.  It finds the K closest points, and takes a vote based on the labels."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's raise K to 3."]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Chat poll: What will our 3 neighbor KNN classifier predict our new point to be?\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["It is a bit harder to tell what which points are closest by eye.\n", "\n", "Let's update our plot to add indexes."]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can the sklearn NearestNeighors object to see the exact calculations."]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Chat poll: What will our 5 neighbor KNN classifier predict our new point to be?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's iterate through K, 1 through 10, and see the predictions."]}, {"cell_type": "markdown", "metadata": {}, "source": ["What K was correct?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Different types of distance"]}, {"cell_type": "markdown", "metadata": {}, "source": ["How did the algo calculate those distances? "]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Euclidean Distance\n", "\n", "**Euclidean distance** refers to the distance between two points. These points can be in different dimensional space and are represented by different forms of coordinates. In one-dimensional space, the points are just on a straight number line.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Measuring distance in a 2-d Space\n", "\n", "In two-dimensional space, the coordinates are given as points on the x- and y-axes\n", "\n", "![alt text](img/euclidean_2d.png)\n", "### Measuring distance in a 3-d Space\n", "\n", "In three-dimensional space, x-, y- and z-axes are used. \n", "\n", "$$\\sqrt{(x_1-x_2)^2 + (y_1-y_2)^2 +  (z_1-z_2)^2}$$\n", "![alt text](img/vectorgraph.jpg)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Manhattan distance\n", "\n", "Manhattan distance is the distance measured if you walked along a city block instead of a straight line. \n", "\n", "> if \ud835\udc65=(\ud835\udc4e,\ud835\udc4f) and \ud835\udc66=(\ud835\udc50,\ud835\udc51),  \n", "> Manhattan distance = |\ud835\udc4e\u2212\ud835\udc50|+|\ud835\udc4f\u2212\ud835\udc51|\n", "\n", "![](img/manhattan.png)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Pairs: \n", "\n", "Write an function that calculates Manhattan distance between two points\n", "\n", "Calculate the distance between new_X and the 15 training points.\n", "\n", "Based on 5 K, determine what decision a KNN algorithm would make if it used Manhattan distance.\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\n", "def manhattan(train_X, index, val_X):\n", "    \"\"\"\n", "    :param train_X: one record from the training set\n", "                    (type series or dataframe including target (survived))\n", "    :param val_X: one record from the validation set\n", "                    series or dataframe include target (survived)\n", "    :return: the Manhattan distance between train_X and val_X\n", "    \"\"\"\n", "    train_X = train_X.loc[index]\n", "    diff = train_X - val_X\n", "    # Remove survived column\n", "    diff = diff.iloc[:, :-1]\n", "    dist = np.abs(diff).sum(axis=1)\n", "    \n", "    return (dist.values[0],index, train_X.Survived)\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["If we change the distance metric, our prediction should change for K = 5."]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Scaling"]}, {"cell_type": "markdown", "metadata": {}, "source": ["You may have suspected that we were leaving something out. For any distance based algorithms, scaling is very important.  Look at how the shape of array changes before and after scaling."]}, {"cell_type": "markdown", "metadata": {}, "source": ["![non-normal](img/nonnormal.png)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["![normal](img/normalized.png)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's look at our data for viz dataset"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Look at how much that changes things.\n", "\n", "Look at 166 to 150.  \n", "Look at the group 621, 143,192"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now let's run our classifier on scaled data and compare to unscaled."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Should we use a Standard Scaler or Min-Max Scaler?  \n", "https://sebastianraschka.com/Articles/2014_about_feature_scaling.html   \n", "http://datareality.blogspot.com/2016/11/scaling-normalizing-standardizing-which.html"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Let's unpack: KNN is a supervised, non-parametric, descriminative, lazy-learning algorithm"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Supervised\n", "You should be very comfortable with the idea of supervised learning by now.  Supervised learning involves labels.  KNN needs labels for the voting process.\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Non-parametric\n", "\n", "Let's look at the fit KNN classifier."]}, {"cell_type": "markdown", "metadata": {}, "source": ["What do you notice? No coefficients! In linear and logistic regression, fitting the model involves calculation of parameters associated with a best fit hyperplane.\n", "\n", "KNN does not use such a process.  It simply calculates the distance from each point, and votes."]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Descriminative\n", "\n", "### Example training data\n", "\n", "This example uses a multi-class problem and each color represents a different class. \n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### KNN classification map (K=1)\n", "\n", "![1NN classification map](img/04_1nn_map.png)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### KNN classification map (K=5)\n", "\n", "![5NN classification map](img/04_5nn_map.png)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## What are those white spaces?\n", "\n", "Those are spaces where ties occur.  \n", "\n", "How can we deal with ties?  \n", "  - for binary classes  \n", "      - choose an odd number for k\n", "        \n", "  - for multiclass  \n", "      - Reduce the K by 1 to see who wins.  \n", "      - Weight the votes based on the distance of the neighbors  "]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Lazy-Learning\n", "![lazy](https://media.giphy.com/media/QSzIZKD16bNeM/giphy.gif)\n", "\n", "Lazy-learning has also to do with KNN's training, or better yet, lack of a training step.  Whereas models like linear and logistic fit onto training data, doing the hard work of calculating paramaters when .fit is called, the training phase of KNN is simply storing the training data in memory.  The training step of KNN takes no time at all. All the work is done in the prediction phase, where the distances are calculated. Prediction is therefore memory intensive, and can take a long time.    KNN is lazy because it puts off the work until a later time than most algos.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Pair \n", "\n", "Use the timeit function to compare the time of fitting and predicting in Logistic vs KNN"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Time it example"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [" \n", "lr = LogisticRegression(max_iter=1000)\n", "%timeit lr.fit(X,y)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\n", "knn = KNeighborsClassifier()\n", "%timeit knn.fit(X,y)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\n", "%timeit knn.predict(X)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Tuning K"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### What value of K performs best on our Test data?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### How do you think K size relates to our concepts of bias and variance?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["![alt text](img/K-NN_Neighborhood_Size_print.png)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.3"}}, "nbformat": 4, "nbformat_minor": 4}