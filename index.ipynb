{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# K-Nearest Neighbors\n", "\n", "![wilson](img/wilson.jpg)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["KNearest Neighbors is our second classification algorithm in our toolbelt added to our logistic regression classifier.\n", "\n", "If we remember, logistic regression is a supervised, parametric, discriminative model.\n", "\n", "KNN is a supervised, non-parametric, discriminative, lazy-learning algorithm.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["mccalister = ['Adam', 'Amanda','Chum', 'Dann',\n", " 'Jacob', 'Jason', 'Johnhoy', 'Karim',\n", "'Leana','Luluva', 'Matt', 'Maximilian', ]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# This is always a good idea\n", "%load_ext autoreload\n", "%autoreload 2\n", "\n", "import os\n", "import sys\n", "module_path = os.path.abspath(os.path.join(os.pardir, os.pardir))\n", "if module_path not in sys.path:\n", "    sys.path.append(module_path)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "from src.student_caller import one_random_student"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Let's load in our trusty Titanic dataset\n", "\n", "![titanic](https://media.giphy.com/media/uhB0n3Eac8ybe/giphy.gif)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["titanic = pd.read_csv('data/cleaned_titanic.csv')\n", "titanic = titanic.iloc[:,:-2]\n", "titanic.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### For visualization purposes, we will use only two features for our first model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X = titanic[['Age', 'Fare']]\n", "y = titanic['Survived']\n", "y.value_counts()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Titanic is a binary classification problem, with our target being the Survived feature"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\n", "\n", "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=42, test_size = .25)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Then perform another tts, and put aside the test set from above until the end\n", "\n", "We will hold of from KFold or crossval for now, so that our notebook is more comprehensible."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_train, X_val, y_train, y_val = train_test_split(X_train,y_train, random_state=42, test_size = .25)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": ["from sklearn.metrics import f1_score, confusion_matrix, recall_score, precision_score\n", "from src.confusion import plot_confusion_matrix\n", "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n", "from sklearn.neighbors import KNeighborsClassifier\n", "\n", "knn = KNeighborsClassifier()\n", "\n", "mm = MinMaxScaler()\n", "X_train = mm.fit_transform(X_train)\n", "X_val = mm.transform(X_val)\n", "\n", "knn.fit(X_train, y_train)\n", "print(f\"training accuracy: {knn.score(X_train, y_train)}\")\n", "print(f\"Val accuracy: {knn.score(X_val, y_val)}\")\n", "\n", "y_hat = knn.predict(X_val)\n", "\n", "plot_confusion_matrix(confusion_matrix(y_val, y_hat), classes=['Perished', 'Survived'])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Quick review of confusion matrix and our metrics: \n", "  "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["question = 'How many true positives?'\n", "one_random_student(mccalister, question)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["question = 'How many true negatives?'\n", "one_random_student(mccalister, question)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["question = 'How many false positives?'\n", "one_random_student(mccalister, question)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["question = 'How many  how many false negatives?'\n", "one_random_student(mccalister, question)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["question = 'Which will be higher: precision or recall'\n", "one_random_student(mccalister, question)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# KNN: Under the Hood"]}, {"cell_type": "markdown", "metadata": {}, "source": ["For visualization purposes, let's pull out a small subset of our training data, and create a model using only two dimensions: Age and Fare.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=42, test_size = .25)\n", "X_train, X_val, y_train, y_val = train_test_split(X_train,y_train, random_state=42, test_size = .25)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import seaborn as sns\n", "\n", "X_for_viz = X_train.sample(15, random_state=40)\n", "y_for_viz = y_train[X_for_viz.index]\n", "\n", "fig, ax = plt.subplots(figsize=(10,10))\n", "sns.scatterplot(X_for_viz['Age'], X_for_viz['Fare'], \n", "                hue=y_for_viz, palette={0:'red', 1:'green'}, \n", "                s=200, ax=ax)\n", "\n", "ax.set_xlim(0,80)\n", "ax.set_ylim(0,80)\n", "plt.legend()\n", "plt.title('Subsample of Training Data')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The KNN algorithm works by simply storing the training set in memory, then measuring the distance from the training points to a a new point.\n", "\n", "Let's drop a point from our validation set into the plot above."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_for_viz = X_train.sample(15, random_state=40)\n", "y_for_viz = y_train[X_for_viz.index]\n", "\n", "fig, ax = plt.subplots(figsize=(10,10))\n", "sns.scatterplot(X_for_viz['Age'], X_for_viz['Fare'], hue=y_for_viz, palette={0:'red', 1:'green'}, s=200, ax=ax)\n", "\n", "plt.legend()\n", "\n", "#################^^^Old code^^^##############\n", "####################New code#################\n", "\n", "# Let's take one sample from our validation set and plot it\n", "new_x = pd.DataFrame(X_val.loc[484]).T\n", "new_y = y_val[new_x.index]\n", "\n", "sns.scatterplot(new_x['Age'], new_x['Fare'], color='blue', s=200, ax=ax, label='New', marker='P')\n", "\n", "ax.set_xlim(0,100)\n", "ax.set_ylim(0,100)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["new_x.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Then, KNN finds the K nearest points. K corresponds to the n_neighbors parameter defined when we instantiate the classifier object."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["knn = KNeighborsClassifier(n_neighbors=1)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's fit our training data, then predict what our validation point will be based on the closest 1 neighbor."]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Chat poll: What will our 1 neighbor KNN classifier predict our new point to be?\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["knn.fit(X_for_viz, y_for_viz)\n", "knn.predict(new_x)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["When we raise the value of K, KNN acts democratically.  It finds the K closest points, and takes a vote based on the labels."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's raise K to 3."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["knn = KNeighborsClassifier(n_neighbors=3)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Chat poll: What will our 3 neighbor KNN classifier predict our new point to be?\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["knn.fit(X_for_viz, y_for_viz)\n", "knn.predict(new_x)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["It is a bit harder to tell what which points are closest by eye.\n", "\n", "Let's update our plot to add indexes."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_for_viz = X_train.sample(15, random_state=40)\n", "y_for_viz = y_train[X_for_viz.index]\n", "\n", "fig, ax = plt.subplots(figsize=(10,10))\n", "sns.scatterplot(X_for_viz['Age'], X_for_viz['Fare'], hue=y_for_viz, \n", "                palette={0:'red', 1:'green'}, s=200, ax=ax)\n", "\n", "\n", "# Now let's take another sample\n", "\n", "# new_x = X_val.sample(1, random_state=33)\n", "new_x = pd.DataFrame(X_val.loc[484]).T\n", "new_x.columns = ['Age','Fare']\n", "new_y = y_val[new_x.index]\n", "\n", "print(new_x)\n", "sns.scatterplot(new_x['Age'], new_x['Fare'], color='blue', s=200, ax=ax, label='New', marker='P')\n", "ax.set_xlim(0,100)\n", "ax.set_ylim(0,100)\n", "plt.legend()\n", "\n", "#################^^^Old code^^^##############\n", "####################New code#################\n", "\n", "# add annotations one by one with a loop\n", "for index in X_for_viz.index:\n", "    ax.text(X_for_viz.Age[index]+0.7, X_for_viz.Fare[index], s=index, horizontalalignment='left', size='medium', color='black', weight='semibold')\n", " \n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can the sklearn NearestNeighors object to see the exact calculations."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.neighbors import NearestNeighbors\n", "\n", "df_for_viz = pd.merge(X_for_viz, y_for_viz, left_index=True, right_index=True)\n", "neighbor = NearestNeighbors(3)\n", "neighbor.fit(X_for_viz)\n", "nearest = neighbor.kneighbors(new_x)\n", "\n", "nearest"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df_for_viz.iloc[nearest[1][0]]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["new_x"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Chat poll: What will our 5 neighbor KNN classifier predict our new point to be?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["knn = KNeighborsClassifier(n_neighbors=5)\n", "knn.fit(X_for_viz, y_for_viz)\n", "knn.predict(new_x)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's iterate through K, 1 through 10, and see the predictions."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for k in range(1,10):\n", "    knn = KNeighborsClassifier(n_neighbors=k)\n", "    knn.fit(X_for_viz, y_for_viz)\n", "    print(knn.predict(new_x))\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["What K was correct?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["new_y"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Different types of distance"]}, {"cell_type": "markdown", "metadata": {}, "source": ["How did the algo calculate those distances? "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["nearest"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Euclidean Distance\n", "\n", "**Euclidean distance** refers to the distance between two points. These points can be in different dimensional space and are represented by different forms of coordinates. In one-dimensional space, the points are just on a straight number line.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Measuring distance in a 2-d Space\n", "\n", "In two-dimensional space, the coordinates are given as points on the x- and y-axes\n", "\n", "![alt text](img/euclidean_2d.png)\n", "### Measuring distance in a 3-d Space\n", "\n", "In three-dimensional space, x-, y- and z-axes are used. \n", "\n", "$$\\sqrt{(x_1-x_2)^2 + (y_1-y_2)^2 +  (z_1-z_2)^2}$$\n", "![alt text](img/vectorgraph.jpg)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Let's reproduce those numbers:\n", "nearest"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df_for_viz.iloc[11]\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["new_x"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def euclid(train_X, val_X):\n", "    \"\"\"\n", "    :param train_X: one record from the training set\n", "                    (type series or dataframe including target (survived))\n", "    :param val_X: one record from the validation set\n", "                    series or dataframe include target (survived)\n", "    :return: The euclidean distance between train_X and val_X\n", "    \"\"\"\n", "    diff = train_X - val_X\n", "\n", "    # Remove survived column\n", "    diff = diff.iloc[:, :-1]\n", "\n", "    dist = np.sqrt((diff ** 2).sum(axis=1))\n", "\n", "    return dist\n", "\n", "    "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["euclid(df_for_viz.iloc[11], new_x)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["euclid(df_for_viz.iloc[5], new_x)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["euclid(df_for_viz.iloc[0], new_x)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Manhattan distance\n", "\n", "Manhattan distance is the distance measured if you walked along a city block instead of a straight line. \n", "\n", "> if \ud835\udc65=(\ud835\udc4e,\ud835\udc4f) and \ud835\udc66=(\ud835\udc50,\ud835\udc51),  \n", "> Manhattan distance = |\ud835\udc4e\u2212\ud835\udc50|+|\ud835\udc4f\u2212\ud835\udc51|\n", "\n", "![](img/manhattan.png)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Pairs: \n", "\n", "Write an function that calculates Manhattan distance between two points\n", "\n", "Calculate the distance between new_X and the 15 training points.\n", "\n", "Based on 5 K, determine what decision a KNN algorithm would make if it used Manhattan distance.\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# your code here\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": ["manh_diffs = []\n", "for index in df_for_viz.index:\n", "    manh_diffs.append(manhattan(df_for_viz,index, new_x))\n", "    \n", "sorted(manh_diffs)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.neighbors import NearestNeighbors\n", "\n", "neighbor = NearestNeighbors(10, p=1)\n", "neighbor.fit(X_for_viz)\n", "nearest = neighbor.kneighbors(new_x)\n", "\n", "nearest"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df_for_viz.iloc[nearest[1][0]]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from src.plot_train import plot_train\n", "plot_train(X_train, y_train, X_val, y_val)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["If we change the distance metric, our prediction should change for K = 5."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.neighbors import KNeighborsClassifier\n", "\n", "knn_euc = KNeighborsClassifier(5, p=2)\n", "knn_euc.fit(X_for_viz, y_for_viz)\n", "knn_euc.predict(new_x)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["knn_man = KNeighborsClassifier(5, p=1)\n", "knn_man.fit(X_for_viz, y_for_viz)\n", "knn_man.predict(new_x)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Which got it right? \n", "new_y"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Scaling"]}, {"cell_type": "markdown", "metadata": {}, "source": ["You may have suspected that we were leaving something out. For any distance based algorithms, scaling is very important.  Look at how the shape of array changes before and after scaling."]}, {"cell_type": "markdown", "metadata": {}, "source": ["![non-normal](img/nonnormal.png)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["![normal](img/normalized.png)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's look at our data for viz dataset"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\n", "\n", "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=42, test_size = .25)\n", "X_train, X_val, y_train, y_val = train_test_split(X_train,y_train, random_state=42, test_size = .25)\n", "\n", "knn = KNeighborsClassifier()\n", "\n", "ss = StandardScaler()\n", "X_ind = X_train.index\n", "X_col = X_train.columns\n", "\n", "X_train_s = pd.DataFrame(ss.fit_transform(X_train))\n", "X_train_s.index = X_ind\n", "X_train_s.columns = X_col\n", "\n", "X_v_ind = X_val.index\n", "X_val_s = pd.DataFrame(ss.transform(X_val))\n", "X_val_s.index = X_v_ind\n", "X_val_s.columns = X_col\n", "\n", "knn.fit(X_train_s, y_train)\n", "print(f\"training accuracy: {knn.score(X_train_s, y_train)}\")\n", "print(f\"Val accuracy: {knn.score(X_val_s, y_val)}\")\n", "\n", "y_hat = knn.predict(X_val_s)\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plot_train(X_train, y_train, X_val, y_val)\n", "plot_train(X_train_s, y_train, X_val_s, y_val, -2.5,2.5, text_pos=.1 )"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Look at how much that changes things.\n", "\n", "Look at 166 to 150.  \n", "Look at the group 621, 143,192"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now let's run our classifier on scaled data and compare to unscaled."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from src.k_classify import predict_one\n", "\n", "titanic = pd.read_csv('data/cleaned_titanic.csv')\n", "X = titanic[['Age', 'Fare']]\n", "y = titanic['Survived']\n", "\n", "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=42, test_size = .25)\n", "X_train, X_val, y_train, y_val = train_test_split(X_train,y_train, random_state=42, test_size = .25)\n", "\n", "predict_one(X_train, X_val, y_train, y_val)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["ss = StandardScaler()\n", "\n", "X_train_s = pd.DataFrame(ss.fit_transform(X_train))\n", "X_train_s.index = X_train.index\n", "X_train_s.columns = X_train.columns\n", "\n", "X_val_s = pd.DataFrame(ss.transform(X_val))\n", "X_val_s.index = X_val.index\n", "X_val_s.columns = X_val.columns\n", "\n", "\n", "predict_one(X_train_s, X_val_s, y_train, y_val)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Should we use a Standard Scaler or Min-Max Scaler?  \n", "https://sebastianraschka.com/Articles/2014_about_feature_scaling.html   \n", "http://datareality.blogspot.com/2016/11/scaling-normalizing-standardizing-which.html"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Let's unpack: KNN is a supervised, non-parametric, descriminative, lazy-learning algorithm"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Supervised\n", "You should be very comfortable with the idea of supervised learning by now.  Supervised learning involves labels.  KNN needs labels for the voting process.\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Non-parametric\n", "\n", "Let's look at the fit KNN classifier."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["knn = KNeighborsClassifier()\n", "knn.__dict__"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["knn.fit(X_train_s, y_train)\n", "knn.__dict__"]}, {"cell_type": "markdown", "metadata": {}, "source": ["What do you notice? No coefficients! In linear and logistic regression, fitting the model involves calculation of parameters associated with a best fit hyperplane.\n", "\n", "KNN does not use such a process.  It simply calculates the distance from each point, and votes."]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Descriminative\n", "\n", "### Example training data\n", "\n", "This example uses a multi-class problem and each color represents a different class. \n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### KNN classification map (K=1)\n", "\n", "![1NN classification map](img/04_1nn_map.png)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### KNN classification map (K=5)\n", "\n", "![5NN classification map](img/04_5nn_map.png)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## What are those white spaces?\n", "\n", "Those are spaces where ties occur.  \n", "\n", "How can we deal with ties?  \n", "  - for binary classes  \n", "      - choose an odd number for k\n", "        \n", "  - for multiclass  \n", "      - Reduce the K by 1 to see who wins.  \n", "      - Weight the votes based on the distance of the neighbors  "]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Lazy-Learning\n", "![lazy](https://media.giphy.com/media/QSzIZKD16bNeM/giphy.gif)\n", "\n", "Lazy-learning has also to do with KNN's training, or better yet, lack of a training step.  Whereas models like linear and logistic fit onto training data, doing the hard work of calculating paramaters when .fit is called, the training phase of KNN is simply storing the training data in memory.  The training step of KNN takes no time at all. All the work is done in the prediction phase, where the distances are calculated. Prediction is therefore memory intensive, and can take a long time.    KNN is lazy because it puts off the work until a later time than most algos.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Pair \n", "\n", "Use the timeit function to compare the time of fitting and predicting in Logistic vs KNN"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Time it example"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%%timeit\n", "import nltk \n", "emma = nltk.corpus.gutenberg.words('austen-emma.txt')\n", "\n", "newlist = []\n", "for word in emma:\n", "    newlist.append(word.upper())\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%timeit newlist = [s.upper() for s in emma]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%timeit newlist = map(str.upper, emma)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Your code here"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Tuning K"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.preprocessing import StandardScaler\n", "from sklearn.preprocessing import MinMaxScaler\n", "from sklearn.metrics import accuracy_score, recall_score, precision_score\n", " "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split, KFold\n", "\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=.25)\n", "# Set test set aside until we are confident in our model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["kf = KFold(n_splits=5)\n", "\n", "k_scores_train = {}\n", "k_scores_val = {}\n", "\n", "\n", "for k in range(1,20):\n", "    knn = KNeighborsClassifier(n_neighbors=k)\n", "    accuracy_score_t = []\n", "    accuracy_score_v = []\n", "    for train_ind, val_ind in kf.split(X_train, y_train):\n", "        \n", "        X_t, y_t = X_train.iloc[train_ind], y_train.iloc[train_ind] \n", "        X_v, y_v = X_train.iloc[val_ind], y_train.iloc[val_ind]\n", "        mm = MinMaxScaler()\n", "        \n", "        X_t_ind = X_t.index\n", "        X_v_ind = X_v.index\n", "        \n", "        X_t = pd.DataFrame(mm.fit_transform(X_t))\n", "        X_t.index = X_t_ind\n", "        X_v = pd.DataFrame(mm.transform(X_v))\n", "        X_v.index = X_v_ind\n", "        \n", "        knn.fit(X_t, y_t)\n", "        \n", "        y_pred_t = knn.predict(X_t)\n", "        y_pred_v = knn.predict(X_v)\n", "        \n", "        accuracy_score_t.append(accuracy_score(y_t, y_pred_t))\n", "        accuracy_score_v.append(accuracy_score(y_v, y_pred_v))\n", "        \n", "        \n", "    k_scores_train[k] = np.mean(accuracy_score_t)\n", "    k_scores_val[k] = np.mean(accuracy_score_v)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["k_scores_train"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["k_scores_val"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots(figsize=(15,15))\n", "\n", "ax.plot(list(k_scores_train.keys()), list(k_scores_train.values()),color='red', linestyle='dashed', marker='o',  \n", "         markerfacecolor='blue', markersize=10, label='Train')\n", "ax.plot(list(k_scores_val.keys()), list(k_scores_val.values()), color='green', linestyle='dashed', marker='o',  \n", "         markerfacecolor='blue', markersize=10, label='Val')\n", "ax.set_xlabel('k')\n", "ax.set_ylabel('Accuracy')\n", "plt.legend()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### What value of K performs best on our Test data?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### How do you think K size relates to our concepts of bias and variance?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["![alt text](img/K-NN_Neighborhood_Size_print.png)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["mm = MinMaxScaler()\n", "\n", "X_train_ind = X_train.index\n", "X_train = pd.DataFrame(mm.fit_transform(X_train))\n", "X_train.index = X_train_ind\n", "\n", "X_test_ind = X_test.index\n", "X_test =  pd.DataFrame(mm.transform(X_test))\n", "X_test.index = X_test_ind\n", "\n", "\n", "knn = KNeighborsClassifier(n_neighbors=9)\n", "knn.fit(X_train, y_train)\n", "\n", "\n", "\n", "print(f\"training accuracy: {knn.score(X_train, y_train)}\")\n", "print(f\"Test accuracy: {knn.score(X_test, y_test)}\")\n", "\n", "y_hat = knn.predict(X_test)\n", "\n", "plot_confusion_matrix(confusion_matrix(y_test, y_hat), classes=['Perished', 'Survived'])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["recall_score(y_test, y_hat)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["precision_score(y_test, y_hat)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.3"}}, "nbformat": 4, "nbformat_minor": 4}